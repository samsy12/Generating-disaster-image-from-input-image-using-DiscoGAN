{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72-ra5l9LhHY",
        "outputId": "98a1ad02-88b1-49b4-de7f-1e711d94fa43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntCzNCK0Ln34"
      },
      "source": [
        "import os\n",
        "os.chdir(\"./drive/MyDrive/discogan/\") "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrg_xui82BWs",
        "outputId": "d50e5f04-3bb6-4802-9db1-33a9033defc4"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/0c22c6754370ba6b1fa8e53bd6e514d4a41a181125d405a501c215cbdbd6/scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "Wd7R9YvV2cXd",
        "outputId": "ec7b8310-201e-4737-ee98-3ba7472bdfa5"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/56/9f67dcd4a4b9960373173a31be1b8c47fe351a1c9385677a7bdd82810e57/ipdb-0.13.9.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.0.0)\n",
            "Collecting ipython>=7.17.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/34/abf879c9c8a282b2d06c880f3791ce91b25de1912bdcfa4a209afebe12f3/ipython-7.24.1-py3-none-any.whl (785kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e6/4b4ca4fa94462d4560ba2f4e62e62108ab07be2e16a92e594e43b12d3300/prompt_toolkit-3.0.18-py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.0.5)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-cp37-none-any.whl size=11649 sha256=c33544432b7c78980aa275a0122903850bdf2092947ffb4c3a3d3c37f44bf402\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/cd/67/bb18bf4caa63b6618f9ec64e8d1d8ba67edcf397cff328fbfb\n",
            "Successfully built ipdb\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.18 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.24.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed ipdb-0.13.9 ipython-7.24.1 prompt-toolkit-3.0.18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrGuBNrgqM1_"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import scipy.io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import ipdb\n",
        "import argparse\n",
        "from itertools import chain\n",
        "import torch.optim as optim\n",
        "from progressbar import ETA, Bar, Percentage, ProgressBar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GdZeSzH0zWo"
      },
      "source": [
        "dataset_path = './dataset/'\n",
        "houses_path = os.path.join(dataset_path, 'houses')\n",
        "disaster_path = os.path.join(dataset_path, 'disasters')\n",
        "\n",
        "def shuffle_data(da, db):\n",
        "    a_idx = list(range(len(da)))\n",
        "    np.random.shuffle( a_idx )\n",
        "\n",
        "    b_idx = list(range(len(db)))\n",
        "    np.random.shuffle(b_idx)\n",
        "\n",
        "    shuffled_da = np.array(da)[ np.array(a_idx) ]\n",
        "    shuffled_db = np.array(db)[ np.array(b_idx) ]\n",
        "\n",
        "    return shuffled_da, shuffled_db\n",
        "\n",
        "def read_images( filenames, domain=None, image_size):\n",
        "\n",
        "    images = []\n",
        "    for fn in filenames:\n",
        "        image = cv2.imread(fn)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        if domain == 'A':\n",
        "            kernel = np.ones((3,3), np.uint8)\n",
        "            image = image[:, :256, :]\n",
        "            image = 255. - image\n",
        "            image = cv2.dilate( image, kernel, iterations=1 )\n",
        "            image = 255. - image\n",
        "        elif domain == 'B':\n",
        "            image = image[:, 256:, :]\n",
        "\n",
        "        image = cv2.resize(image, (image_size,image_size))\n",
        "        image = image.astype(np.float32) / 255.\n",
        "        image = image.transpose(2,0,1)\n",
        "        images.append( image )\n",
        "\n",
        "    images = np.stack( images )\n",
        "    return images\n",
        "\n",
        "\n",
        "def get_edge2photo_files(item='houses', test=False):\n",
        "    if item == 'houses':\n",
        "        item_path = houses_path\n",
        "    elif item == 'disasters':\n",
        "        item_path = disasters_path\n",
        "\n",
        "    if test == True:\n",
        "        item_path = os.path.join( item_path, 'val' )\n",
        "    else:\n",
        "        item_path = os.path.join( item_path, 'train' )\n",
        "\n",
        "    image_paths = map(lambda x: os.path.join( item_path, x ), os.listdir( item_path ))\n",
        "\n",
        "    if test == True:\n",
        "        return [list(image_paths), list(image_paths)]\n",
        "    else:\n",
        "        image_paths=list(image_paths)\n",
        "        n_images = len( image_paths)\n",
        "        return [image_paths[:int(n_images/2)], image_paths[int(n_images/2):]]\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba87aNfMz-1E"
      },
      "source": [
        "kernel_sizes = [4,3,3]\n",
        "strides = [2,2,1]\n",
        "paddings=[0,0,1]\n",
        "\n",
        "latent_dim = 300\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            ):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n",
        "        self.relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64 * 2)\n",
        "        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(64 * 4)\n",
        "        self.relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(64 * 8)\n",
        "        self.relu4 = nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv1 = self.conv1( input )\n",
        "        relu1 = self.relu1( conv1 )\n",
        "\n",
        "        conv2 = self.conv2( relu1 )\n",
        "        bn2 = self.bn2( conv2 )\n",
        "        relu2 = self.relu2( bn2 )\n",
        "\n",
        "        conv3 = self.conv3( relu2 )\n",
        "        bn3 = self.bn3( conv3 )\n",
        "        relu3 = self.relu3( bn3 )\n",
        "\n",
        "        conv4 = self.conv4( relu3 )\n",
        "        bn4 = self.bn4( conv4 )\n",
        "        relu4 = self.relu4( bn4 )\n",
        "\n",
        "        conv5 = self.conv5( relu4 )\n",
        "\n",
        "        return torch.sigmoid( conv5 ), [relu2, relu3, relu4]\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            extra_layers=False\n",
        "            ):\n",
        "\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        if extra_layers == True:\n",
        "            self.main = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 2),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 4),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 8),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(100),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(64 * 8),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 4),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 2),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64 * 2,     64, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(    64,      3, 4, 2, 1, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "\n",
        "        if extra_layers == False:\n",
        "            self.main = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 2),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 4),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 8),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 4),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64 * 2),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(64 * 2,     64, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(True),\n",
        "                nn.ConvTranspose2d(    64,      3, 4, 2, 1, bias=False),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main( input )\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs5JLWMsv-kq"
      },
      "source": [
        "cuda='true'\n",
        "epoch_size=5000\n",
        "batch_size=64\n",
        "learning_rate=0.0002\n",
        "result_path='./results/'\n",
        "model_path='./models/'\n",
        "model_arch='discogan'\n",
        "image_size=64\n",
        "starting_rate=0.01\n",
        "default_rate=0.5\n",
        "update_erval=3\n",
        "log_erval=50\n",
        "image_save_erval=1000\n",
        "model_save_erval=10000\n",
        "\n",
        "def as_np(data):\n",
        "    return data.cpu().data.numpy()\n",
        "\n",
        "def get_data():\n",
        "    \n",
        "    data_A_1, data_A_2 = get_edge2photo_files( item='houses', test=False )\n",
        "    test_A_1, test_A_2 = get_edge2photo_files( item='houses', test=True )\n",
        "\n",
        "    data_A = np.hstack( [data_A_1, data_A_2] )\n",
        "    test_A = np.hstack( [test_A_1, test_A_2] )\n",
        "\n",
        "    data_B_1, data_B_2 = get_edge2photo_files( item='disasters', test=False )\n",
        "    test_B_1, test_B_2 = get_edge2photo_files( item='disasters', test=True )\n",
        "\n",
        "    data_B = np.hstack( [data_B_1, data_B_2] )\n",
        "    test_B = np.hstack( [test_B_1, test_B_2] )\n",
        "\n",
        "    return data_A, data_B, test_A, test_B\n",
        "\n",
        "def get_fm_loss(real_feats, fake_feats, criterion):\n",
        "    losses = 0\n",
        "    for real_feat, fake_feat in zip(real_feats, fake_feats):\n",
        "        l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))\n",
        "        loss = criterion( l2, Variable( torch.ones( l2.size() ) ).cuda() )\n",
        "        losses += loss\n",
        "\n",
        "    return losses\n",
        "\n",
        "def get_gan_loss(dis_real, dis_fake, criterion, cuda=False):\n",
        "    labels_dis_real = Variable(torch.ones( [dis_real.size()[0], 1] ))\n",
        "    labels_dis_fake = Variable(torch.zeros([dis_fake.size()[0], 1] ))\n",
        "    labels_gen = Variable(torch.ones([dis_fake.size()[0], 1]))\n",
        "\n",
        "    if cuda:\n",
        "        labels_dis_real = labels_dis_real.cuda()\n",
        "        labels_dis_fake = labels_dis_fake.cuda()\n",
        "        labels_gen = labels_gen.cuda()\n",
        "\n",
        "    dis_loss = criterion( dis_real.resize(64,1), labels_dis_real.resize(64,1) ) * 0.5 + criterion( dis_fake.resize(64,1), labels_dis_fake.resize(64,1) ) * 0.5\n",
        "    gen_loss = criterion( dis_fake, labels_gen )\n",
        "\n",
        "    return dis_loss, gen_loss\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    global args\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    cuda = True\n",
        "    \n",
        "    #cuda = False\n",
        "\n",
        "\n",
        "    epoch_size = 10000\n",
        "    batch_size = 64\n",
        "\n",
        "    result_path = os.path.join( './results/', 'houses2disaster' )\n",
        "    result_path = os.path.join( result_path, 'discogan' )\n",
        "\n",
        "    model_path = os.path.join( './models/', 'houses2disaster' )\n",
        "    \n",
        "    model_path = os.path.join( model_path, 'discogan' )\n",
        "\n",
        "    data_style_A, data_style_B, test_style_A, test_style_B = get_data()\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    test_A = read_images( test_style_A, 'B', image_size )\n",
        "    test_B = read_images( test_style_B, 'B', image_size )\n",
        "\n",
        "\n",
        "    test_A = Variable( torch.FloatTensor( test_A ), volatile=True )\n",
        "    test_B = Variable( torch.FloatTensor( test_B ), volatile=True )\n",
        "\n",
        "    if not os.path.exists(result_path):\n",
        "        os.makedirs(result_path)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "\n",
        "    generator_A = Generator()\n",
        "    generator_B = Generator()\n",
        "    discriminator_A = Discriminator()\n",
        "    discriminator_B = Discriminator()\n",
        "\n",
        "    if cuda:\n",
        "        test_A = test_A.cuda()\n",
        "        test_B = test_B.cuda()\n",
        "        generator_A = generator_A.cuda()\n",
        "        generator_B = generator_B.cuda()\n",
        "        discriminator_A = discriminator_A.cuda()\n",
        "        discriminator_B = discriminator_B.cuda()\n",
        "\n",
        "    data_size = min( len(data_style_A), len(data_style_B) )\n",
        "    n_batches = ( data_size // batch_size )\n",
        "\n",
        "    recon_criterion = nn.MSELoss()\n",
        "    gan_criterion = nn.BCELoss()\n",
        "    feat_criterion = nn.HingeEmbeddingLoss()\n",
        "\n",
        "    gen_params = chain(generator_A.parameters(), generator_B.parameters())\n",
        "    dis_params = chain(discriminator_A.parameters(), discriminator_B.parameters())\n",
        "\n",
        "    optim_gen = optim.Adam( gen_params, lr=learning_rate, betas=(0.5,0.999), weight_decay=0.00001)\n",
        "    optim_dis = optim.Adam( dis_params, lr=learning_rate, betas=(0.5,0.999), weight_decay=0.00001)\n",
        "\n",
        "    iters = 0\n",
        "\n",
        "    gen_loss_total = []\n",
        "    dis_loss_total = []\n",
        "\n",
        "    for epoch in range(epoch_size):\n",
        "        data_style_A, data_style_B = shuffle_data( data_style_A, data_style_B)\n",
        "\n",
        "        widgets = ['epoch #%d|' % epoch, Percentage(), Bar(), ETA()]\n",
        "        pbar = ProgressBar(maxval=n_batches, widgets=widgets)\n",
        "        pbar.start()\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            print(\"iter:\",iter)\n",
        "\n",
        "            pbar.update(i)\n",
        "\n",
        "            generator_A.zero_grad()\n",
        "            generator_B.zero_grad()\n",
        "            discriminator_A.zero_grad()\n",
        "            discriminator_B.zero_grad()\n",
        "\n",
        "            A_path = data_style_A[ i * batch_size: (i+1) * batch_size ]\n",
        "            B_path = data_style_B[ i * batch_size: (i+1) * batch_size ]\n",
        "\n",
        "            \n",
        "            A = read_images( A_path, 'B', image_size )\n",
        "            B = read_images( B_path, 'B', image_size )\n",
        "            \n",
        "\n",
        "            A = Variable( torch.FloatTensor( A ) )\n",
        "            B = Variable( torch.FloatTensor( B ) )\n",
        "\n",
        "            if cuda:\n",
        "                A = A.cuda()\n",
        "                B = B.cuda()\n",
        "\n",
        "            AB = generator_B(A)\n",
        "            BA = generator_A(B)\n",
        "\n",
        "            ABA = generator_A(AB)\n",
        "            BAB = generator_B(BA)\n",
        "\n",
        "            # Reconstruction Loss\n",
        "            recon_loss_A = recon_criterion( ABA, A )\n",
        "            recon_loss_B = recon_criterion( BAB, B )\n",
        "\n",
        "            # Real/Fake GAN Loss (A)\n",
        "            A_dis_real, A_feats_real = discriminator_A( A )\n",
        "            A_dis_fake, A_feats_fake = discriminator_A( BA )\n",
        "\n",
        "            dis_loss_A, gen_loss_A = get_gan_loss( A_dis_real, A_dis_fake.resize(64,1), gan_criterion, cuda )\n",
        "            fm_loss_A = get_fm_loss(A_feats_real, A_feats_fake, feat_criterion)\n",
        "\n",
        "            # Real/Fake GAN Loss (B)\n",
        "            B_dis_real, B_feats_real = discriminator_B( B )\n",
        "            B_dis_fake, B_feats_fake = discriminator_B( AB )\n",
        "\n",
        "            dis_loss_B, gen_loss_B = get_gan_loss( B_dis_real, B_dis_fake.resize(64,1), gan_criterion, cuda )\n",
        "            fm_loss_B = get_fm_loss( B_feats_real, B_feats_fake, feat_criterion )\n",
        "\n",
        "            # Total Loss\n",
        "\n",
        "            if iters < gan_curriculum:\n",
        "                rate = starting_rate\n",
        "            else:\n",
        "                rate = default_rate\n",
        "\n",
        "            gen_loss_A_total = (gen_loss_B*0.1 + fm_loss_B*0.9) * (1.-rate) + recon_loss_A * rate\n",
        "            gen_loss_B_total = (gen_loss_A*0.1 + fm_loss_A*0.9) * (1.-rate) + recon_loss_B * rate\n",
        "\n",
        "            \n",
        "            gen_loss = gen_loss_A_total + gen_loss_B_total\n",
        "            dis_loss = dis_loss_A + dis_loss_B\n",
        "            \n",
        "\n",
        "            if iters % update_interval == 0:\n",
        "                dis_loss.backward()\n",
        "                optim_dis.step()\n",
        "            else:\n",
        "                gen_loss.backward()\n",
        "                optim_gen.step()\n",
        "\n",
        "            if iters % log_interval == 0:\n",
        "                print (\"---------------------\")\n",
        "                print (\"GEN Loss:\", as_np(gen_loss_A.mean()), as_np(gen_loss_B.mean()))\n",
        "                print (\"DIS Loss:\", as_np(dis_loss_A.mean()), as_np(dis_loss_B.mean()))\n",
        "\n",
        "            if iters % image_save_interval == 0:\n",
        "                AB = generator_B( test_A )\n",
        "                BA = generator_A( test_B )\n",
        "                ABA = generator_A( AB )\n",
        "                BAB = generator_B( BA )\n",
        "\n",
        "                n_testset = min( test_A.size()[0], test_B.size()[0] )\n",
        "\n",
        "                subdir_path = os.path.join( result_path, str(iters / image_save_interval) )\n",
        "\n",
        "                if os.path.exists( subdir_path ):\n",
        "                    pass\n",
        "                else:\n",
        "                    os.makedirs( subdir_path )\n",
        "\n",
        "                for im_idx in range( n_testset ):\n",
        "                    A_val = test_A[im_idx].cpu().data.numpy().transpose(1,2,0) * 255.\n",
        "                    B_val = test_B[im_idx].cpu().data.numpy().transpose(1,2,0) * 255.\n",
        "                    BA_val = BA[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.\n",
        "                    ABA_val = ABA[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.\n",
        "                    AB_val = AB[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.\n",
        "                    BAB_val = BAB[im_idx].cpu().data.numpy().transpose(1,2,0)* 255.\n",
        "\n",
        "                    filename_prefix = os.path.join (subdir_path, str(im_idx))\n",
        "                    scipy.misc.imsave( filename_prefix + '.A.jpg', A_val.astype(np.uint8)[:,:,::-1])\n",
        "                    scipy.misc.imsave( filename_prefix + '.B.jpg', B_val.astype(np.uint8)[:,:,::-1])\n",
        "                    scipy.misc.imsave( filename_prefix + '.BA.jpg', BA_val.astype(np.uint8)[:,:,::-1])\n",
        "                    scipy.misc.imsave( filename_prefix + '.AB.jpg', AB_val.astype(np.uint8)[:,:,::-1])\n",
        "                    scipy.misc.imsave( filename_prefix + '.ABA.jpg', ABA_val.astype(np.uint8)[:,:,::-1])\n",
        "                    scipy.misc.imsave( filename_prefix + '.BAB.jpg', BAB_val.astype(np.uint8)[:,:,::-1])\n",
        "                    #print(\"saving imgs\")\n",
        "\n",
        "            if iters % 10000 == 0:\n",
        "                torch.save( generator_A, os.path.join(model_path, 'model_gen_A-' + str( iters / model_save_interval )))\n",
        "                torch.save( generator_B, os.path.join(model_path, 'model_gen_B-' + str( iters / model_save_interval )))\n",
        "                torch.save( discriminator_A, os.path.join(model_path, 'model_dis_A-' + str( iters / model_save_interval )))\n",
        "                torch.save( discriminator_B, os.path.join(model_path, 'model_dis_B-' + str( iters / model_save_interval )))\n",
        "\n",
        "            iters += 1\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
